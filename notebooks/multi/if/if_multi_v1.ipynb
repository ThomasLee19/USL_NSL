{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and set paths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Define paths\n",
    "processed_train_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTrain_processed.csv'\n",
    "processed_test_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTest_processed.csv'\n",
    "train_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTrain_labels.csv'\n",
    "test_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTest_labels.csv'\n",
    "\n",
    "# Load class names mapping\n",
    "preprocessing_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/preprocessing_objects.pkl'\n",
    "with open(preprocessing_path, 'rb') as f:\n",
    "    preprocessing_objects = pickle.load(f)\n",
    "    class_names = preprocessing_objects['class_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Class distribution in training data:\n",
      "Class 0 (Normal Traffic): 67343 samples (53.46%)\n",
      "Class 1 (DOS (Denial of Service)): 45927 samples (36.46%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 11656 samples (9.25%)\n",
      "Class 3 (R2L (Remote to Local)): 995 samples (0.79%)\n",
      "Class 4 (U2R (User to Root)): 52 samples (0.04%)\n",
      "\n",
      "Dataset shapes:\n",
      "Training set: (100778, 43)\n",
      "Validation set: (25195, 43)\n",
      "Loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and prepare data\n",
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_csv(processed_train_path)\n",
    "X = df_train.drop('multiclass_label', axis=1).values\n",
    "y = df_train['multiclass_label'].values\n",
    "\n",
    "# Split training set and validation set (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display number of samples for each class\n",
    "class_dist = pd.Series(y).value_counts().sort_index()\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "for class_id, count in class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(y)*100:.2f}%)\")\n",
    "\n",
    "# Display data information\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(\"Loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training multiclass Isolation Forest models...\n",
      "Number of classes: 5\n",
      "Contamination parameters for each class:\n",
      "Class 0 (Normal Traffic): 0.1000\n",
      "Class 1 (DOS (Denial of Service)): 0.5000\n",
      "Class 2 (Probe (Surveillance/Scanning)): 0.5000\n",
      "Class 3 (R2L (Remote to Local)): 0.5000\n",
      "Class 4 (U2R (User to Root)): 0.5000\n",
      "\n",
      "Training model for class 0 (Normal Traffic)...\n",
      "Using 53874 normal samples for training\n",
      "\n",
      "Training model for class 1 (DOS (Denial of Service))...\n",
      "Using 53874 normal samples and 36741 samples of class 1 for training\n",
      "\n",
      "Training model for class 2 (Probe (Surveillance/Scanning))...\n",
      "Using 46625 normal samples and 9325 samples of class 2 for training\n",
      "\n",
      "Training model for class 3 (R2L (Remote to Local))...\n",
      "Using 3980 normal samples and 796 samples of class 3 for training\n",
      "\n",
      "Training model for class 4 (U2R (User to Root))...\n",
      "Using 1000 normal samples and 42 samples of class 4 for training\n",
      "\n",
      "All models have been trained!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Training multiclass Isolation Forest models\n",
    "print(\"\\nTraining multiclass Isolation Forest models...\")\n",
    "\n",
    "# Get number of classes\n",
    "n_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Adjust contamination parameters based on the proportion of each class in the data\n",
    "class_proportions = pd.Series(y_train).value_counts(normalize=True).sort_index()\n",
    "contaminations = {}\n",
    "for i in range(n_classes):\n",
    "    if i == 0:\n",
    "        contaminations[i] = 0.1\n",
    "    else:\n",
    "        contaminations[i] = min(0.5, 1 - class_proportions[i])\n",
    "        \n",
    "print(\"Contamination parameters for each class:\")\n",
    "for i in range(n_classes):\n",
    "    print(f\"Class {i} ({class_names[i]}): {contaminations[i]:.4f}\")\n",
    "\n",
    "# Train an Isolation Forest model for each class\n",
    "isolation_forests = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    print(f\"\\nTraining model for class {i} ({class_names[i]})...\")\n",
    "    \n",
    "    if i == 0:\n",
    "        current_class_samples = X_train[y_train == i]\n",
    "        print(f\"Using {len(current_class_samples)} normal samples for training\")\n",
    "    else:\n",
    "        # Select samples from the normal class\n",
    "        normal_samples = X_train[y_train == 0]\n",
    "        abnormal_samples = X_train[y_train == i]\n",
    "        \n",
    "        # Keep a reasonable ratio between normal and abnormal samples\n",
    "        n_normal = min(len(normal_samples), max(len(abnormal_samples) * 5, 1000))\n",
    "        \n",
    "        # Select normal samples randomly\n",
    "        normal_indices = np.random.choice(len(normal_samples), n_normal, replace=False)\n",
    "        selected_normal = normal_samples[normal_indices]\n",
    "        \n",
    "        # Combine samples\n",
    "        current_class_samples = np.vstack([selected_normal, abnormal_samples])\n",
    "        print(f\"Using {len(selected_normal)} normal samples and {len(abnormal_samples)} samples of class {i} for training\")\n",
    "    \n",
    "    # Train the Isolation Forest model for the current class\n",
    "    iso_forest = IsolationForest(\n",
    "        n_estimators=200,\n",
    "        max_samples='auto',\n",
    "        contamination=contaminations[i],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    iso_forest.fit(current_class_samples)\n",
    "    isolation_forests.append(iso_forest)\n",
    "\n",
    "print(\"\\nAll models have been trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on training set...\n",
      "\n",
      "Training set accuracy: 0.4310\n",
      "Macro-average precision: 0.5521\n",
      "Macro-average recall: 0.5789\n",
      "Macro-average F1-score: 0.3768\n",
      "\n",
      "Classification Report (Training Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.02      0.04     53874\n",
      "           1       0.97      0.91      0.94     36741\n",
      "           2       0.75      0.91      0.82      9325\n",
      "           3       0.05      0.90      0.09       796\n",
      "           4       0.00      0.17      0.00        42\n",
      "\n",
      "    accuracy                           0.43    100778\n",
      "   macro avg       0.55      0.58      0.38    100778\n",
      "weighted avg       0.95      0.43      0.44    100778\n",
      "\n",
      "\n",
      "Confusion Matrix (Training Set):\n",
      "[[ 1010   417  2165 14348 35934]\n",
      " [    1 33252   555   599  2334]\n",
      " [    3   517  8447    16   342]\n",
      " [    3     7    46   715    25]\n",
      " [    4     3     0    28     7]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Evaluate on training set\n",
    "print(\"\\nEvaluating on training set...\")\n",
    "\n",
    "# Calculate anomaly scores for each class\n",
    "train_scores = np.zeros((X_train.shape[0], n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calculate anomaly scores for the i-th class\n",
    "    train_scores[:, i] = -isolation_forests[i].score_samples(X_train)  # Negative sign makes lower scores indicate more likely to belong to that class\n",
    "\n",
    "# Predict the class with the lowest anomaly score\n",
    "train_predictions = np.argmin(train_scores, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(y_train, train_predictions, average='macro')\n",
    "train_report = classification_report(y_train, train_predictions)\n",
    "train_confusion = confusion_matrix(y_train, train_predictions)\n",
    "\n",
    "print(f\"\\nTraining set accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {train_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {train_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {train_f1:.4f}\")\n",
    "print(\"\\nClassification Report (Training Set):\")\n",
    "print(train_report)\n",
    "print(\"\\nConfusion Matrix (Training Set):\")\n",
    "print(train_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n",
      "\n",
      "Validation set accuracy: 0.4282\n",
      "Macro-average precision: 0.5525\n",
      "Macro-average recall: 0.5458\n",
      "Macro-average F1-score: 0.3762\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.02      0.04     13469\n",
      "           1       0.97      0.90      0.93      9186\n",
      "           2       0.76      0.91      0.82      2331\n",
      "           3       0.05      0.90      0.09       199\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.43     25195\n",
      "   macro avg       0.55      0.55      0.38     25195\n",
      "weighted avg       0.95      0.43      0.44     25195\n",
      "\n",
      "\n",
      "Confusion Matrix (Validation Set):\n",
      "[[ 241  107  527 3621 8973]\n",
      " [   1 8251  149  153  632]\n",
      " [   0  112 2117    3   99]\n",
      " [   1    1    9  180    8]\n",
      " [   1    0    1    8    0]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "\n",
    "# Calculate anomaly scores for each class\n",
    "val_scores = np.zeros((X_val.shape[0], n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calculate anomaly scores for the i-th class\n",
    "    val_scores[:, i] = -isolation_forests[i].score_samples(X_val)  # Negative sign makes lower scores indicate more likely to belong to that class\n",
    "\n",
    "# Predict the class with the lowest anomaly score\n",
    "val_predictions = np.argmin(val_scores, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(y_val, val_predictions, average='macro')\n",
    "val_report = classification_report(y_val, val_predictions)\n",
    "val_confusion = confusion_matrix(y_val, val_predictions)\n",
    "\n",
    "print(f\"\\nValidation set accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {val_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {val_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {val_f1:.4f}\")\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(val_report)\n",
    "print(\"\\nConfusion Matrix (Validation Set):\")\n",
    "print(val_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Class distribution in test data:\n",
      "Class 0 (Normal Traffic): 9711 samples (43.08%)\n",
      "Class 1 (DOS (Denial of Service)): 7458 samples (33.08%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 2421 samples (10.74%)\n",
      "Class 3 (R2L (Remote to Local)): 2887 samples (12.81%)\n",
      "Class 4 (U2R (User to Root)): 67 samples (0.30%)\n",
      "\n",
      "Test set accuracy: 0.3986\n",
      "Macro-average precision: 0.4855\n",
      "Macro-average recall: 0.3742\n",
      "Macro-average F1-score: 0.3064\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.05      9711\n",
      "           1       0.63      0.85      0.72      7458\n",
      "           2       0.66      0.46      0.54      2421\n",
      "           3       0.13      0.45      0.21      2887\n",
      "           4       0.01      0.09      0.01        67\n",
      "\n",
      "    accuracy                           0.40     22544\n",
      "   macro avg       0.49      0.37      0.31     22544\n",
      "weighted avg       0.73      0.40      0.34     22544\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[ 237 1185  320 7160  809]\n",
      " [   1 6338   74 1001   44]\n",
      " [   0 1207 1112   93    9]\n",
      " [   0 1309  164 1293  121]\n",
      " [   0   12   14   35    6]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv(processed_test_path)\n",
    "X_test = df_test.drop('multiclass_label', axis=1).values\n",
    "y_test = df_test['multiclass_label'].values\n",
    "\n",
    "# Display distribution of classes in test data\n",
    "test_class_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "print(\"\\nClass distribution in test data:\")\n",
    "for class_id, count in test_class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Calculate anomaly scores for each class\n",
    "test_scores = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calculate anomaly scores for the i-th class\n",
    "    test_scores[:, i] = -isolation_forests[i].score_samples(X_test)\n",
    "\n",
    "# Predict the class with the lowest anomaly score\n",
    "test_predictions = np.argmin(test_scores, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(y_test, test_predictions, average='macro')\n",
    "test_report = classification_report(y_test, test_predictions)\n",
    "test_confusion = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nTest set accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {test_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {test_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {test_f1:.4f}\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(test_report)\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(test_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl-nsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
