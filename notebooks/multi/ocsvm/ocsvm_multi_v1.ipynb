{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup Paths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cupy as cp\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Define paths\n",
    "processed_train_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTrain_processed.csv'\n",
    "processed_test_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTest_processed.csv'\n",
    "train_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTrain_labels.csv'\n",
    "test_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTest_labels.csv'\n",
    "\n",
    "# Load class names mapping\n",
    "preprocessing_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/preprocessing_objects.pkl'\n",
    "with open(preprocessing_path, 'rb') as f:\n",
    "    preprocessing_objects = pickle.load(f)\n",
    "    class_names = preprocessing_objects['class_names']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Class distribution in training data:\n",
      "Class 0 (Normal Traffic): 67343 samples (53.46%)\n",
      "Class 1 (DOS (Denial of Service)): 45927 samples (36.46%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 11656 samples (9.25%)\n",
      "Class 3 (R2L (Remote to Local)): 995 samples (0.79%)\n",
      "Class 4 (U2R (User to Root)): 52 samples (0.04%)\n",
      "\n",
      "Dataset shapes:\n",
      "Training set: (100778, 43)\n",
      "Validation set: (25195, 43)\n",
      "Loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Prepare Data\n",
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_csv(processed_train_path)\n",
    "X = df_train.drop('multiclass_label', axis=1).values\n",
    "y = df_train['multiclass_label'].values\n",
    "\n",
    "# Split training set and validation set (80-20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Display number of samples for each class\n",
    "class_dist = pd.Series(y).value_counts().sort_index()\n",
    "print(\"\\nClass distribution in training data:\")\n",
    "for class_id, count in class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(y)*100:.2f}%)\")\n",
    "\n",
    "# Display data information\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(\"Loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training multiclass One-Class SVM models...\n",
      "Number of classes: 5\n",
      "Nu parameters for each class:\n",
      "Class 0 (Normal Traffic): 0.1000\n",
      "Class 1 (DOS (Denial of Service)): 0.2000\n",
      "Class 2 (Probe (Surveillance/Scanning)): 0.2000\n",
      "Class 3 (R2L (Remote to Local)): 0.2000\n",
      "Class 4 (U2R (User to Root)): 0.2000\n",
      "\n",
      "Training model for class 0 (Normal Traffic)...\n",
      "Using 53874 normal samples for training\n",
      "Training completed in 61.21 seconds\n",
      "\n",
      "Training model for class 1 (DOS (Denial of Service))...\n",
      "Using 53874 normal samples and 36741 samples of class 1 for training\n",
      "Training completed in 581.59 seconds\n",
      "\n",
      "Training model for class 2 (Probe (Surveillance/Scanning))...\n",
      "Using 46625 normal samples and 9325 samples of class 2 for training\n",
      "Training completed in 143.18 seconds\n",
      "\n",
      "Training model for class 3 (R2L (Remote to Local))...\n",
      "Using 3980 normal samples and 796 samples of class 3 for training\n",
      "Training completed in 0.56 seconds\n",
      "\n",
      "Training model for class 4 (U2R (User to Root))...\n",
      "Using 1000 normal samples and 42 samples of class 4 for training\n",
      "Training completed in 0.03 seconds\n",
      "\n",
      "All models have been trained! Total training time: 786.56 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Training multiclass One-Class SVM models\n",
    "print(\"\\nTraining multiclass One-Class SVM models...\")\n",
    "\n",
    "# Get number of classes\n",
    "n_classes = len(np.unique(y))\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Adjust nu parameters based on the proportion of each class in the data\n",
    "# nu is an upper bound on the fraction of training errors and a lower bound on the fraction of support vectors\n",
    "class_proportions = pd.Series(y_train).value_counts(normalize=True).sort_index()\n",
    "nu_params = {}\n",
    "for i in range(n_classes):\n",
    "    if i == 0:\n",
    "        nu_params[i] = 0.1  # Normal traffic class\n",
    "    else:\n",
    "        nu_params[i] = min(0.2, 1 - class_proportions[i])\n",
    "        \n",
    "print(\"Nu parameters for each class:\")\n",
    "for i in range(n_classes):\n",
    "    print(f\"Class {i} ({class_names[i]}): {nu_params[i]:.4f}\")\n",
    "\n",
    "# Train a One-Class SVM model for each class\n",
    "oc_svms = []\n",
    "training_times = []\n",
    "\n",
    "for i in range(n_classes):\n",
    "    print(f\"\\nTraining model for class {i} ({class_names[i]})...\")\n",
    "    \n",
    "    if i == 0:\n",
    "        # For normal traffic, train only on normal samples\n",
    "        current_class_samples = X_train[y_train == i]\n",
    "        print(f\"Using {len(current_class_samples)} normal samples for training\")\n",
    "    else:\n",
    "        # For anomaly classes, train on normal samples and samples of the current class\n",
    "        normal_samples = X_train[y_train == 0]\n",
    "        abnormal_samples = X_train[y_train == i]\n",
    "        \n",
    "        # Keep a reasonable ratio between normal and abnormal samples\n",
    "        n_normal = min(len(normal_samples), max(len(abnormal_samples) * 5, 1000))\n",
    "        \n",
    "        # Select normal samples randomly\n",
    "        normal_indices = np.random.choice(len(normal_samples), n_normal, replace=False)\n",
    "        selected_normal = normal_samples[normal_indices]\n",
    "        \n",
    "        # Combine samples\n",
    "        current_class_samples = np.vstack([selected_normal, abnormal_samples])\n",
    "        print(f\"Using {len(selected_normal)} normal samples and {len(abnormal_samples)} samples of class {i} for training\")\n",
    "    \n",
    "    # Train the One-Class SVM model for the current class\n",
    "    start_time = time.time()\n",
    "    oc_svm = OneClassSVM(\n",
    "        kernel='rbf',\n",
    "        nu=nu_params[i],\n",
    "        gamma='scale',\n",
    "        cache_size=500,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    oc_svm.fit(current_class_samples)\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    training_times.append(training_time)\n",
    "    \n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    oc_svms.append(oc_svm)\n",
    "\n",
    "print(f\"\\nAll models have been trained! Total training time: {sum(training_times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on training set...\n",
      "\n",
      "Training set accuracy: 0.4244\n",
      "Macro-average precision: 0.7530\n",
      "Macro-average recall: 0.3560\n",
      "Macro-average F1-score: 0.3164\n",
      "\n",
      "Classification Report (Training Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.01      0.01     53874\n",
      "           1       0.39      1.00      0.56     36741\n",
      "           2       0.83      0.61      0.70      9325\n",
      "           3       0.95      0.07      0.13       796\n",
      "           4       1.00      0.10      0.17        42\n",
      "\n",
      "    accuracy                           0.42    100778\n",
      "   macro avg       0.75      0.36      0.32    100778\n",
      "weighted avg       0.55      0.42      0.28    100778\n",
      "\n",
      "\n",
      "Confusion Matrix (Training Set):\n",
      "[[  319 52458  1094     3     0]\n",
      " [    0 36697    44     0     0]\n",
      " [    0  3628  5697     0     0]\n",
      " [  202   501    38    55     0]\n",
      " [   15    18     5     0     4]]\n",
      "\n",
      "Evaluating on validation set...\n",
      "\n",
      "Validation set accuracy: 0.4243\n",
      "Macro-average precision: 0.6629\n",
      "Macro-average recall: 0.3497\n",
      "Macro-average F1-score: 0.3045\n",
      "\n",
      "Classification Report (Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.01      0.01     13469\n",
      "           1       0.39      1.00      0.56      9186\n",
      "           2       0.81      0.61      0.70      2331\n",
      "           3       0.54      0.04      0.07       199\n",
      "           4       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.42     25195\n",
      "   macro avg       0.66      0.35      0.30     25195\n",
      "weighted avg       0.53      0.42      0.28     25195\n",
      "\n",
      "\n",
      "Confusion Matrix (Validation Set):\n",
      "[[   95 13073   296     5     0]\n",
      " [    0  9169    17     0     0]\n",
      " [    0   914  1417     0     0]\n",
      " [   69   114     9     7     0]\n",
      " [    3     2     3     1     1]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Evaluate on Training and Validation Sets\n",
    "print(\"\\nEvaluating on training set...\")\n",
    "\n",
    "# Calculate anomaly scores for each class\n",
    "train_scores = np.zeros((X_train.shape[0], n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calculate anomaly scores for the i-th class\n",
    "    train_scores[:, i] = -oc_svms[i].score_samples(X_train)\n",
    "\n",
    "# Predict the class with the lowest anomaly score\n",
    "train_predictions = np.argmin(train_scores, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(y_train, train_predictions, average='macro')\n",
    "train_report = classification_report(y_train, train_predictions)\n",
    "train_confusion = confusion_matrix(y_train, train_predictions)\n",
    "\n",
    "print(f\"\\nTraining set accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {train_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {train_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {train_f1:.4f}\")\n",
    "print(\"\\nClassification Report (Training Set):\")\n",
    "print(train_report)\n",
    "print(\"\\nConfusion Matrix (Training Set):\")\n",
    "print(train_confusion)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "\n",
    "# Calculate anomaly scores for each class\n",
    "val_scores = np.zeros((X_val.shape[0], n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calculate anomaly scores for the i-th class\n",
    "    val_scores[:, i] = -oc_svms[i].score_samples(X_val)\n",
    "\n",
    "# Predict the class with the lowest anomaly score\n",
    "val_predictions = np.argmin(val_scores, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "val_accuracy = accuracy_score(y_val, val_predictions)\n",
    "val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(y_val, val_predictions, average='macro')\n",
    "val_report = classification_report(y_val, val_predictions)\n",
    "val_confusion = confusion_matrix(y_val, val_predictions)\n",
    "\n",
    "print(f\"\\nValidation set accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {val_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {val_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {val_f1:.4f}\")\n",
    "print(\"\\nClassification Report (Validation Set):\")\n",
    "print(val_report)\n",
    "print(\"\\nConfusion Matrix (Validation Set):\")\n",
    "print(val_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Class distribution in test data:\n",
      "Class 0 (Normal Traffic): 9711 samples (43.08%)\n",
      "Class 1 (DOS (Denial of Service)): 7458 samples (33.08%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 2421 samples (10.74%)\n",
      "Class 3 (R2L (Remote to Local)): 2887 samples (12.81%)\n",
      "Class 4 (U2R (User to Root)): 67 samples (0.30%)\n",
      "\n",
      "Test set accuracy: 0.3660\n",
      "Macro-average precision: 0.6031\n",
      "Macro-average recall: 0.2927\n",
      "Macro-average F1-score: 0.2416\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.00      0.00      9711\n",
      "           1       0.35      1.00      0.52      7458\n",
      "           2       0.70      0.33      0.45      2421\n",
      "           3       0.26      0.00      0.01      2887\n",
      "           4       1.00      0.13      0.24        67\n",
      "\n",
      "    accuracy                           0.37     22544\n",
      "   macro avg       0.60      0.29      0.24     22544\n",
      "weighted avg       0.53      0.37      0.22     22544\n",
      "\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "[[  10 9569  118   14    0]\n",
      " [   0 7424   34    0    0]\n",
      " [   0 1623  798    0    0]\n",
      " [   3 2698  177    9    0]\n",
      " [   1   26   19   12    9]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluate on Test Set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "# Load test data\n",
    "df_test = pd.read_csv(processed_test_path)\n",
    "X_test = df_test.drop('multiclass_label', axis=1).values\n",
    "y_test = df_test['multiclass_label'].values\n",
    "\n",
    "# Display distribution of classes in test data\n",
    "test_class_dist = pd.Series(y_test).value_counts().sort_index()\n",
    "print(\"\\nClass distribution in test data:\")\n",
    "for class_id, count in test_class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(y_test)*100:.2f}%)\")\n",
    "\n",
    "# Calculate anomaly scores for each class\n",
    "test_scores = np.zeros((X_test.shape[0], n_classes))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    # Calculate anomaly scores for the i-th class\n",
    "    test_scores[:, i] = -oc_svms[i].score_samples(X_test)\n",
    "\n",
    "# Predict the class with the lowest anomaly score\n",
    "test_predictions = np.argmin(test_scores, axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(y_test, test_predictions, average='macro')\n",
    "test_report = classification_report(y_test, test_predictions)\n",
    "test_confusion = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "print(f\"\\nTest set accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {test_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {test_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {test_f1:.4f}\")\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(test_report)\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(test_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl-nsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
