{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Setup Paths\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define paths for binary classification (OneClassSVM)\n",
    "bin_processed_train_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/bin/KDDTrain_processed.csv'\n",
    "bin_processed_test_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/bin/KDDTest_processed.csv'\n",
    "bin_train_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/bin/KDDTrain_labels.csv'\n",
    "bin_test_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/bin/KDDTest_labels.csv'\n",
    "\n",
    "# Define paths for multi-class classification (XGBoost)\n",
    "multi_processed_train_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTrain_processed.csv'\n",
    "multi_processed_test_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTest_processed.csv'\n",
    "multi_train_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTrain_labels.csv'\n",
    "multi_test_labels_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/KDDTest_labels.csv'\n",
    "\n",
    "# Load class names mapping\n",
    "preprocessing_path = '/root/autodl-tmp/projects/USL_NSL/dataset/processed/multi/preprocessing_objects.pkl'\n",
    "with open(preprocessing_path, 'rb') as f:\n",
    "    preprocessing_objects = pickle.load(f)\n",
    "    class_names = preprocessing_objects['class_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading binary classification data for OneClassSVM...\n",
      "Applying SMOTE to handle class imbalance for binary data...\n",
      "Binary training set shape: (107748, 43)\n",
      "Binary validation set shape: (26938, 43)\n",
      "Binary data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load and Prepare Binary Data for OneClassSVM\n",
    "print(\"Loading binary classification data for OneClassSVM...\")\n",
    "# Load binary features and labels\n",
    "df_bin_train = pd.read_csv(bin_processed_train_path)\n",
    "X_bin = df_bin_train.values\n",
    "\n",
    "y_bin_train = pd.read_csv(bin_train_labels_path)\n",
    "y_bin_train_binary = y_bin_train['label'].values\n",
    "\n",
    "# Feature scaling for binary data\n",
    "bin_scaler = StandardScaler()\n",
    "X_bin_scaled = bin_scaler.fit_transform(X_bin)\n",
    "\n",
    "# Handle class imbalance using SMOTE for binary classification\n",
    "print(\"Applying SMOTE to handle class imbalance for binary data...\")\n",
    "bin_smote = SMOTE(random_state=42)\n",
    "X_bin_resampled, y_bin_resampled = bin_smote.fit_resample(X_bin_scaled, y_bin_train_binary)\n",
    "\n",
    "# Split binary data into training and validation sets\n",
    "X_bin_train, X_bin_val, y_bin_train_split, y_bin_val = train_test_split(\n",
    "    X_bin_resampled, y_bin_resampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Binary training set shape: {X_bin_train.shape}\")\n",
    "print(f\"Binary validation set shape: {X_bin_val.shape}\")\n",
    "print(\"Binary data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading multi-class data for XGBoost...\n",
      "\n",
      "Class distribution in multi-class training data:\n",
      "Class 0 (Normal Traffic): 67343 samples (53.46%)\n",
      "Class 1 (DOS (Denial of Service)): 45927 samples (36.46%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 11656 samples (9.25%)\n",
      "Class 3 (R2L (Remote to Local)): 995 samples (0.79%)\n",
      "Class 4 (U2R (User to Root)): 52 samples (0.04%)\n",
      "\n",
      "Multi-class training set shape: (100778, 43)\n",
      "Multi-class validation set shape: (25195, 43)\n",
      "Multi-class data loading complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Prepare Multi-class Data for XGBoost\n",
    "print(\"\\nLoading multi-class data for XGBoost...\")\n",
    "# Load multi-class features and labels\n",
    "df_multi_train = pd.read_csv(multi_processed_train_path)\n",
    "X_multi = df_multi_train.drop('multiclass_label', axis=1).values\n",
    "y_multi = df_multi_train['multiclass_label'].values\n",
    "\n",
    "# Create binary labels (0: normal, 1: attack) for reference\n",
    "y_multi_binary = np.where(y_multi == 0, 0, 1)\n",
    "\n",
    "# Split multi-class data into training and validation sets\n",
    "X_multi_train, X_multi_val, y_multi_train, y_multi_val = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi\n",
    ")\n",
    "_, _, y_multi_binary_train, y_multi_binary_val = train_test_split(\n",
    "    X_multi, y_multi_binary, test_size=0.2, random_state=42, stratify=y_multi_binary\n",
    ")\n",
    "\n",
    "# Feature scaling for multi-class data\n",
    "multi_scaler = StandardScaler()\n",
    "X_multi_train_scaled = multi_scaler.fit_transform(X_multi_train)\n",
    "X_multi_val_scaled = multi_scaler.transform(X_multi_val)\n",
    "\n",
    "# Display number of samples for each class\n",
    "class_dist = pd.Series(y_multi).value_counts().sort_index()\n",
    "print(\"\\nClass distribution in multi-class training data:\")\n",
    "for class_id, count in class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(y_multi)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nMulti-class training set shape: {X_multi_train.shape}\")\n",
    "print(f\"Multi-class validation set shape: {X_multi_val.shape}\")\n",
    "print(\"Multi-class data loading complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training OneClassSVM for anomaly detection using binary data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/sl-nsl-env/lib/python3.10/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneClassSVM training completed in 272.16 seconds\n",
      "\n",
      "Unsupervised Evaluation Metrics:\n",
      "Silhouette Score: 0.241\n",
      "Davies-Bouldin Index: 2.728\n",
      "\n",
      "Prediction distribution (Binary Validation Set):\n",
      "{0: 16823, 1: 10115}\n",
      "\n",
      "Anomaly detector - Binary validation set performance:\n",
      "Accuracy: 0.5284\n",
      "Precision: 0.5418\n",
      "Recall: 0.4045\n",
      "F1-score: 0.4632\n",
      "\n",
      "Confusion Matrix (Binary Validation Set):\n",
      "True Negatives: 8754 | False Positives: 4635\n",
      "False Negatives: 8069 | True Positives: 5480\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Train OneClassSVM for Anomaly Detection (Using Binary Data)\n",
    "print(\"\\nTraining OneClassSVM for anomaly detection using binary data...\")\n",
    "\n",
    "# Train OneClassSVM with optimized parameters\n",
    "start_time = time.time()\n",
    "anomaly_detector = OneClassSVM(\n",
    "    kernel='rbf',\n",
    "    nu=0.35,           # Parameter from the successful binary model\n",
    "    gamma='scale',     # Kernel coefficient\n",
    "    cache_size=1000,   # Increased cache size\n",
    "    max_iter=1000,     # Increased max iterations\n",
    "    tol=1e-4           # Set a smaller tolerance for convergence\n",
    ")\n",
    "\n",
    "# Train on the binary training data\n",
    "anomaly_detector.fit(X_bin_train)\n",
    "anomaly_train_time = time.time() - start_time\n",
    "print(f\"OneClassSVM training completed in {anomaly_train_time:.2f} seconds\")\n",
    "\n",
    "# Function to convert OneClassSVM predictions to binary labels\n",
    "def convert_predictions(predictions):\n",
    "    return np.where(predictions == 1, 0, 1)\n",
    "\n",
    "# Evaluate anomaly detector on binary validation set\n",
    "anomaly_val_pred_raw = anomaly_detector.predict(X_bin_val)\n",
    "anomaly_val_pred = convert_predictions(anomaly_val_pred_raw)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "anomaly_val_accuracy = accuracy_score(y_bin_val, anomaly_val_pred)\n",
    "anomaly_val_precision, anomaly_val_recall, anomaly_val_f1, _ = precision_recall_fscore_support(\n",
    "    y_bin_val, anomaly_val_pred, average='binary'\n",
    ")\n",
    "\n",
    "# Calculate unsupervised evaluation metrics\n",
    "try:\n",
    "    silhouette_avg = silhouette_score(X_bin_val, anomaly_val_pred)\n",
    "    davies_bouldin_idx = davies_bouldin_score(X_bin_val, anomaly_val_pred)\n",
    "    print(\"\\nUnsupervised Evaluation Metrics:\")\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    print(f\"Davies-Bouldin Index: {davies_bouldin_idx:.3f}\")\n",
    "except:\n",
    "    print(\"Could not calculate unsupervised metrics due to single class prediction\")\n",
    "\n",
    "# Print prediction distribution\n",
    "print(\"\\nPrediction distribution (Binary Validation Set):\")\n",
    "unique, counts = np.unique(anomaly_val_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "print(f\"\\nAnomaly detector - Binary validation set performance:\")\n",
    "print(f\"Accuracy: {anomaly_val_accuracy:.4f}\")\n",
    "print(f\"Precision: {anomaly_val_precision:.4f}\")\n",
    "print(f\"Recall: {anomaly_val_recall:.4f}\")\n",
    "print(f\"F1-score: {anomaly_val_f1:.4f}\")\n",
    "\n",
    "# Display confusion matrix for binary classification\n",
    "anomaly_val_cm = confusion_matrix(y_bin_val, anomaly_val_pred)\n",
    "print(\"\\nConfusion Matrix (Binary Validation Set):\")\n",
    "print(f\"True Negatives: {anomaly_val_cm[0, 0]} | False Positives: {anomaly_val_cm[0, 1]}\")\n",
    "print(f\"False Negatives: {anomaly_val_cm[1, 0]} | True Positives: {anomaly_val_cm[1, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost for attack type classification using multi-class data...\n",
      "Performing feature selection...\n",
      "Applying adaptive sampling to handle class imbalance...\n",
      "Original attack class distribution:\n",
      "[36741  9325   796    42]\n",
      "\n",
      "Attack class distribution after sampling:\n",
      "[36741 11190  1592   126]\n",
      "Attack classifier training completed in 1445.44 seconds\n",
      "\n",
      "Attack classifier - Training set performance (attack samples only):\n",
      "Accuracy: 0.9991\n",
      "Macro-average precision: 0.9595\n",
      "Macro-average recall: 0.9170\n",
      "Macro-average F1-score: 0.9361\n",
      "\n",
      "Classification Report (Training Set) - Attack Types:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     36741\n",
      "           1       1.00      1.00      1.00      9325\n",
      "           2       0.99      0.98      0.98       796\n",
      "           3       0.85      0.69      0.76        42\n",
      "\n",
      "    accuracy                           1.00     46904\n",
      "   macro avg       0.96      0.92      0.94     46904\n",
      "weighted avg       1.00      1.00      1.00     46904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Train XGBoost for Attack Type Classification (Using Multi-class Data)\n",
    "print(\"\\nTraining XGBoost for attack type classification using multi-class data...\")\n",
    "\n",
    "# Extract attack samples for training\n",
    "attack_indices_train = np.where(y_multi_train != 0)[0]\n",
    "X_attack_train = X_multi_train[attack_indices_train]\n",
    "y_attack_train = y_multi_train[attack_indices_train]\n",
    "# Shift labels to start from 0 (1->0, 2->1, 3->2, 4->3)\n",
    "y_attack_train_shifted = y_attack_train - 1\n",
    "\n",
    "# Feature selection for attack classification\n",
    "print(\"Performing feature selection...\")\n",
    "selector = SelectKBest(f_classif, k=40)\n",
    "X_attack_train_selected = selector.fit_transform(X_attack_train, y_attack_train_shifted)\n",
    "\n",
    "# Define adaptive sampling function for attack types\n",
    "def adaptive_sampling(X, y, verbose=True):\n",
    "    # Calculate class distributions\n",
    "    class_counts = np.bincount(y)\n",
    "    if verbose:\n",
    "        print(\"Original attack class distribution:\")\n",
    "        print(class_counts)\n",
    "    \n",
    "    # Define sampling strategy for attack types\n",
    "    sampling_strategy = {\n",
    "        0: int(class_counts[0] * 1.0),  # DOS\n",
    "        1: int(class_counts[1] * 1.2),  # Probe\n",
    "        2: int(class_counts[2] * 2.0),  # R2L\n",
    "        3: int(class_counts[3] * 3.0)   # U2R\n",
    "    }\n",
    "    \n",
    "    # Apply SMOTE with adaptive strategy\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=sampling_strategy,\n",
    "        random_state=42,\n",
    "        k_neighbors=min(5, min(class_counts[class_counts > 0]) - 1)\n",
    "    )\n",
    "    \n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nAttack class distribution after sampling:\")\n",
    "        print(np.bincount(y_resampled))\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply adaptive sampling to handle class imbalance\n",
    "print(\"Applying adaptive sampling to handle class imbalance...\")\n",
    "X_attack_train_balanced, y_attack_train_balanced = adaptive_sampling(\n",
    "    X_attack_train_selected, y_attack_train_shifted\n",
    ")\n",
    "\n",
    "# Train XGBoost for attack type classification\n",
    "start_time = time.time()\n",
    "\n",
    "# Create base XGBoost model\n",
    "base_model = XGBClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=6,\n",
    "    min_child_weight=5,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=2,\n",
    "    objective='multi:softmax',\n",
    "    num_class=4,  # 4 attack types (excluding normal)\n",
    "    random_state=42,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "# Use Bagging to improve model performance\n",
    "attack_classifier = BaggingClassifier(\n",
    "    estimator=base_model,\n",
    "    n_estimators=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "attack_classifier.fit(X_attack_train_balanced, y_attack_train_balanced)\n",
    "attack_train_time = time.time() - start_time\n",
    "print(f\"Attack classifier training completed in {attack_train_time:.2f} seconds\")\n",
    "\n",
    "# Evaluate attack classifier on attack samples from training set\n",
    "y_attack_train_pred = attack_classifier.predict(X_attack_train_selected)\n",
    "attack_train_accuracy = accuracy_score(y_attack_train_shifted, y_attack_train_pred)\n",
    "attack_train_precision, attack_train_recall, attack_train_f1, _ = precision_recall_fscore_support(\n",
    "    y_attack_train_shifted, y_attack_train_pred, average='macro'\n",
    ")\n",
    "\n",
    "print(f\"\\nAttack classifier - Training set performance (attack samples only):\")\n",
    "print(f\"Accuracy: {attack_train_accuracy:.4f}\")\n",
    "print(f\"Macro-average precision: {attack_train_precision:.4f}\")\n",
    "print(f\"Macro-average recall: {attack_train_recall:.4f}\")\n",
    "print(f\"Macro-average F1-score: {attack_train_f1:.4f}\")\n",
    "\n",
    "# Display classification report for attack types\n",
    "attack_train_report = classification_report(y_attack_train_shifted, y_attack_train_pred)\n",
    "print(\"\\nClassification Report (Training Set) - Attack Types:\")\n",
    "print(attack_train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating hybrid model on multi-class validation set...\n",
      "\n",
      "Hybrid model - Multi-class validation set performance:\n",
      "Accuracy: 0.4658\n",
      "Macro-average precision: 0.4296\n",
      "Macro-average recall: 0.7350\n",
      "Macro-average F1-score: 0.3501\n",
      "\n",
      "Classification Report (Multi-class Validation Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.00      0.00     13469\n",
      "           1       0.51      1.00      0.68      9186\n",
      "           2       0.56      1.00      0.72      2331\n",
      "           3       0.06      0.97      0.12       199\n",
      "           4       0.14      0.70      0.23        10\n",
      "\n",
      "    accuracy                           0.47     25195\n",
      "   macro avg       0.43      0.73      0.35     25195\n",
      "weighted avg       0.71      0.47      0.32     25195\n",
      "\n",
      "\n",
      "Confusion Matrix (Multi-class Validation Set):\n",
      "[[  27 8729 1791 2880   42]\n",
      " [   4 9179    3    0    0]\n",
      " [   0    1 2328    2    0]\n",
      " [   0    0    3  194    2]\n",
      " [   0    2    1    0    7]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluate Hybrid Model on Multi-class Validation Set\n",
    "print(\"\\nEvaluating hybrid model on multi-class validation set...\")\n",
    "\n",
    "# Step 1: Use OneClassSVM to detect anomalies\n",
    "# First, we need to scale the multi-class validation data using the binary scaler\n",
    "X_multi_val_bin_scaled = bin_scaler.transform(X_multi_val)\n",
    "anomaly_multi_val_pred_raw = anomaly_detector.predict(X_multi_val_bin_scaled)\n",
    "anomaly_multi_val_pred = convert_predictions(anomaly_multi_val_pred_raw)\n",
    "\n",
    "# Step 2: For samples classified as anomalies, predict the attack type\n",
    "anomaly_indices_val = np.where(anomaly_multi_val_pred == 1)[0]\n",
    "X_anomaly_multi_val = X_multi_val[anomaly_indices_val]\n",
    "\n",
    "if len(anomaly_indices_val) > 0:\n",
    "    # Apply feature selection\n",
    "    X_anomaly_multi_val_selected = selector.transform(X_anomaly_multi_val)\n",
    "    \n",
    "    # Predict attack types\n",
    "    y_attack_multi_val_pred = attack_classifier.predict(X_anomaly_multi_val_selected)\n",
    "    # Shift back to original labels (0->1, 1->2, 2->3, 3->4)\n",
    "    y_attack_multi_val_pred_shifted = y_attack_multi_val_pred + 1\n",
    "    \n",
    "    # Create final predictions\n",
    "    y_multi_val_pred = np.zeros_like(y_multi_val)\n",
    "    y_multi_val_pred[anomaly_indices_val] = y_attack_multi_val_pred_shifted\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    val_accuracy = accuracy_score(y_multi_val, y_multi_val_pred)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(y_multi_val, y_multi_val_pred, average='macro')\n",
    "    val_report = classification_report(y_multi_val, y_multi_val_pred)\n",
    "    val_confusion = confusion_matrix(y_multi_val, y_multi_val_pred)\n",
    "    \n",
    "    print(f\"\\nHybrid model - Multi-class validation set performance:\")\n",
    "    print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Macro-average precision: {val_precision:.4f}\")\n",
    "    print(f\"Macro-average recall: {val_recall:.4f}\")\n",
    "    print(f\"Macro-average F1-score: {val_f1:.4f}\")\n",
    "    print(\"\\nClassification Report (Multi-class Validation Set):\")\n",
    "    print(val_report)\n",
    "    print(\"\\nConfusion Matrix (Multi-class Validation Set):\")\n",
    "    print(val_confusion)\n",
    "else:\n",
    "    print(\"No anomalies detected in validation set by OneClassSVM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating hybrid model on multi-class test set...\n",
      "\n",
      "Class distribution in multi-class test data:\n",
      "Class 0 (Normal Traffic): 9711 samples (43.08%)\n",
      "Class 1 (DOS (Denial of Service)): 7458 samples (33.08%)\n",
      "Class 2 (Probe (Surveillance/Scanning)): 2421 samples (10.74%)\n",
      "Class 3 (R2L (Remote to Local)): 2887 samples (12.81%)\n",
      "Class 4 (U2R (User to Root)): 67 samples (0.30%)\n",
      "\n",
      "Anomaly detector - Test set performance (binary):\n",
      "Accuracy: 0.5706\n",
      "Precision: 0.5700\n",
      "Recall: 0.9998\n",
      "F1-score: 0.7261\n",
      "\n",
      "Confusion Matrix (Test Set) - Binary Classification:\n",
      "True Negatives: 33 | False Positives: 9678\n",
      "False Negatives: 2 | True Positives: 12831\n",
      "\n",
      "Hybrid model - Multi-class test set performance:\n",
      "Accuracy: 0.3785\n",
      "Macro-average precision: 0.6539\n",
      "Macro-average recall: 0.3992\n",
      "Macro-average F1-score: 0.3173\n",
      "\n",
      "Classification Report (Multi-class Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.00      0.01      9711\n",
      "           1       0.39      0.85      0.54      7458\n",
      "           2       0.30      0.73      0.42      2421\n",
      "           3       0.98      0.13      0.22      2887\n",
      "           4       0.66      0.28      0.40        67\n",
      "\n",
      "    accuracy                           0.38     22544\n",
      "   macro avg       0.65      0.40      0.32     22544\n",
      "weighted avg       0.70      0.38      0.26     22544\n",
      "\n",
      "\n",
      "Confusion Matrix (Multi-class Test Set):\n",
      "[[  33 7608 2059    5    6]\n",
      " [   2 6344 1112    0    0]\n",
      " [   0  647 1774    0    0]\n",
      " [   0 1520 1000  363    4]\n",
      " [   0   32   14    2   19]]\n",
      "\n",
      "Per-class performance:\n",
      "Class 0 (Normal Traffic) accuracy: 0.0034\n",
      "Class 1 (DOS (Denial of Service)) accuracy: 0.8506\n",
      "Class 2 (Probe (Surveillance/Scanning)) accuracy: 0.7328\n",
      "Class 3 (R2L (Remote to Local)) accuracy: 0.1257\n",
      "Class 4 (U2R (User to Root)) accuracy: 0.2836\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Evaluate Hybrid Model on Multi-class Test Set\n",
    "print(\"\\nEvaluating hybrid model on multi-class test set...\")\n",
    "\n",
    "# Load multi-class test data\n",
    "df_multi_test = pd.read_csv(multi_processed_test_path)\n",
    "X_multi_test = df_multi_test.drop('multiclass_label', axis=1).values\n",
    "y_multi_test = df_multi_test['multiclass_label'].values\n",
    "\n",
    "# Create binary labels from multi-class labels for binary evaluation\n",
    "y_multi_binary_test = np.where(y_multi_test == 0, 0, 1)\n",
    "\n",
    "# Display distribution of classes in test data\n",
    "test_class_dist = pd.Series(y_multi_test).value_counts().sort_index()\n",
    "print(\"\\nClass distribution in multi-class test data:\")\n",
    "for class_id, count in test_class_dist.items():\n",
    "    print(f\"Class {class_id} ({class_names[class_id]}): {count} samples ({count/len(y_multi_test)*100:.2f}%)\")\n",
    "\n",
    "# Scale multi-class test data using binary scaler for anomaly detection\n",
    "X_multi_test_bin_scaled = bin_scaler.transform(X_multi_test)\n",
    "\n",
    "# Step 1: Use OneClassSVM to detect anomalies\n",
    "anomaly_multi_test_pred_raw = anomaly_detector.predict(X_multi_test_bin_scaled)\n",
    "anomaly_multi_test_pred = convert_predictions(anomaly_multi_test_pred_raw)\n",
    "\n",
    "# Calculate binary classification metrics\n",
    "anomaly_test_accuracy = accuracy_score(y_multi_binary_test, anomaly_multi_test_pred)\n",
    "anomaly_test_precision, anomaly_test_recall, anomaly_test_f1, _ = precision_recall_fscore_support(\n",
    "    y_multi_binary_test, anomaly_multi_test_pred, average='binary'\n",
    ")\n",
    "\n",
    "print(f\"\\nAnomaly detector - Test set performance (binary):\")\n",
    "print(f\"Accuracy: {anomaly_test_accuracy:.4f}\")\n",
    "print(f\"Precision: {anomaly_test_precision:.4f}\")\n",
    "print(f\"Recall: {anomaly_test_recall:.4f}\")\n",
    "print(f\"F1-score: {anomaly_test_f1:.4f}\")\n",
    "\n",
    "# Display confusion matrix for binary classification\n",
    "anomaly_test_cm = confusion_matrix(y_multi_binary_test, anomaly_multi_test_pred)\n",
    "print(\"\\nConfusion Matrix (Test Set) - Binary Classification:\")\n",
    "print(f\"True Negatives: {anomaly_test_cm[0, 0]} | False Positives: {anomaly_test_cm[0, 1]}\")\n",
    "print(f\"False Negatives: {anomaly_test_cm[1, 0]} | True Positives: {anomaly_test_cm[1, 1]}\")\n",
    "\n",
    "# Step 2: For samples classified as anomalies, predict the attack type\n",
    "anomaly_indices_test = np.where(anomaly_multi_test_pred == 1)[0]\n",
    "X_anomaly_multi_test = X_multi_test[anomaly_indices_test]\n",
    "\n",
    "if len(anomaly_indices_test) > 0:\n",
    "    # Apply feature selection\n",
    "    X_anomaly_multi_test_selected = selector.transform(X_anomaly_multi_test)\n",
    "    \n",
    "    # Predict attack types\n",
    "    y_attack_multi_test_pred = attack_classifier.predict(X_anomaly_multi_test_selected)\n",
    "    # Shift back to original labels\n",
    "    y_attack_multi_test_pred_shifted = y_attack_multi_test_pred + 1\n",
    "    \n",
    "    # Create final predictions\n",
    "    y_multi_test_pred = np.zeros_like(y_multi_test)\n",
    "    y_multi_test_pred[anomaly_indices_test] = y_attack_multi_test_pred_shifted\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    test_accuracy = accuracy_score(y_multi_test, y_multi_test_pred)\n",
    "    test_precision, test_recall, test_f1, _ = precision_recall_fscore_support(y_multi_test, y_multi_test_pred, average='macro')\n",
    "    test_report = classification_report(y_multi_test, y_multi_test_pred)\n",
    "    test_confusion = confusion_matrix(y_multi_test, y_multi_test_pred)\n",
    "    \n",
    "    print(f\"\\nHybrid model - Multi-class test set performance:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Macro-average precision: {test_precision:.4f}\")\n",
    "    print(f\"Macro-average recall: {test_recall:.4f}\")\n",
    "    print(f\"Macro-average F1-score: {test_f1:.4f}\")\n",
    "    print(\"\\nClassification Report (Multi-class Test Set):\")\n",
    "    print(test_report)\n",
    "    print(\"\\nConfusion Matrix (Multi-class Test Set):\")\n",
    "    print(test_confusion)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    print(\"\\nPer-class performance:\")\n",
    "    for i in range(5):  # 0-4 classes\n",
    "        class_indices = np.where(y_multi_test == i)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_acc = accuracy_score(y_multi_test[class_indices], y_multi_test_pred[class_indices])\n",
    "            print(f\"Class {i} ({class_names[i]}) accuracy: {class_acc:.4f}\")\n",
    "else:\n",
    "    print(\"No anomalies detected in test set by OneClassSVM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving models...\n",
      "Models saved to /root/autodl-tmp/projects/USL_NSL/notebooks/multi/hybrid/models/improved_hybrid_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Save Trained Models\n",
    "print(\"\\nSaving models...\")\n",
    "# Create Paths to Save Models\n",
    "model_dir = '/root/autodl-tmp/projects/USL_NSL/notebooks/multi/hybrid/models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    print(f\"Created directory: {model_dir}\")\n",
    "\n",
    "models = {\n",
    "    'bin_scaler': bin_scaler,\n",
    "    'multi_scaler': multi_scaler,\n",
    "    'anomaly_detector': anomaly_detector,\n",
    "    'attack_classifier': attack_classifier,\n",
    "    'feature_selector': selector,\n",
    "    'class_names': class_names\n",
    "}\n",
    "\n",
    "model_path = '/root/autodl-tmp/projects/USL_NSL/notebooks/multi/hybrid/models/improved_hybrid_model.pkl'\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(models, f)\n",
    "print(f\"Models saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl-nsl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
